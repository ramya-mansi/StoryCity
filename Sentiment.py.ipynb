{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063527d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (6.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f047d384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad613557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (3.6.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.22.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.25->seaborn) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: keras in c:\\users\\ramya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d00faf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94f261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/ramya/OneDrive/Desktop/Projects/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "991f7602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2339a9b08b</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16fab9f95b</td>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>like</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74a76f6e0a</td>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>DANGERously</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>04dd1d2e34</td>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bbe3cbf620</td>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textID                                               text  \\\n",
       "0   cb774db0d1                I`d have responded, if I were going   \n",
       "1   549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2   088c60f138                          my boss is bullying me...   \n",
       "3   9642c003ef                     what interview! leave me alone   \n",
       "4   358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5   28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6   6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7   50e14c0bb8                                         Soooo high   \n",
       "8   e050245fbd                                        Both of you   \n",
       "9   fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "10  2339a9b08b   as much as i love to be hopeful, i reckon the...   \n",
       "11  16fab9f95b  I really really like the song Love Story by Ta...   \n",
       "12  74a76f6e0a       My Sharpie is running DANGERously low on ink   \n",
       "13  04dd1d2e34  i want to go to music tonight but i lost my vo...   \n",
       "14  bbe3cbf620                         test test from the LG enV2   \n",
       "\n",
       "                                        selected_text sentiment  \n",
       "0                 I`d have responded, if I were going   neutral  \n",
       "1                                            Sooo SAD  negative  \n",
       "2                                         bullying me  negative  \n",
       "3                                      leave me alone  negative  \n",
       "4                                       Sons of ****,  negative  \n",
       "5   http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                 fun  positive  \n",
       "7                                          Soooo high   neutral  \n",
       "8                                         Both of you   neutral  \n",
       "9                        Wow... u just became cooler.  positive  \n",
       "10  as much as i love to be hopeful, i reckon the ...   neutral  \n",
       "11                                               like  positive  \n",
       "12                                        DANGERously  negative  \n",
       "13                                               lost  negative  \n",
       "14                         test test from the LG enV2   neutral  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1b009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's get the dataset lenght\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baca3034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is there any other different value than neutral, negative and positive?\n",
    "train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed473ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "      <td>7781</td>\n",
       "      <td>5861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11118</td>\n",
       "      <td>11117</td>\n",
       "      <td>11111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "      <td>8582</td>\n",
       "      <td>5537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID   text  selected_text\n",
       "sentiment                              \n",
       "negative     7781   7781           5861\n",
       "neutral     11118  11117          11111\n",
       "positive     8582   8582           5537"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How's distributed the dataset? Is it biased?\n",
    "train.groupby('sentiment').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d7fea",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abeb3d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         selected_text sentiment\n",
       "0  I`d have responded, if I were going   neutral\n",
       "1                             Sooo SAD  negative\n",
       "2                          bullying me  negative\n",
       "3                       leave me alone  negative\n",
       "4                        Sons of ****,  negative"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's keep only the columns that we're going to use\n",
    "train = train[['selected_text','sentiment']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d01c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is there any null value?\n",
    "train[\"selected_text\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2294360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fill the only null value.\n",
    "train[\"selected_text\"].fillna(\"No content\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7b951fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depure_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0b78c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I`d have responded, if I were going',\n",
       " 'Sooo SAD',\n",
       " 'bullying me',\n",
       " 'leave me alone',\n",
       " 'Sons of ****,']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "#Splitting pd.Series to list\n",
    "data_to_list = train['selected_text'].values.tolist()\n",
    "for i in range(len(data_to_list)):\n",
    "    temp.append(depure_data(data_to_list[i]))\n",
    "list(temp[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69bc383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['have', 'responded', 'if', 'were', 'going'], ['sooo', 'sad'], ['bullying', 'me'], ['leave', 'me', 'alone'], ['sons', 'of'], ['some', 'shameless', 'plugging', 'for', 'the', 'best', 'rangers', 'forum', 'on', 'earth'], ['fun'], ['soooo', 'high'], ['both', 'of', 'you'], ['wow', 'just', 'became', 'cooler']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "\n",
    "data_words = list(sent_to_words(temp))\n",
    "\n",
    "print(data_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ae8893f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28ceb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ab5a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have responded if were going', 'sooo sad', 'bullying me', 'leave me alone', 'sons of']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(data_words)):\n",
    "    data.append(detokenize(data_words[i]))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fe4df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665dc27",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae6c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(train['sentiment'])\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 'neutral':\n",
    "        y.append(0)\n",
    "    if labels[i] == 'negative':\n",
    "        y.append(1)\n",
    "    if labels[i] == 'positive':\n",
    "        y.append(2)\n",
    "y = np.array(y)\n",
    "labels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a4c109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a08ad6",
   "metadata": {},
   "source": [
    "## Data sequencing and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a5036de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   68  146   41]\n",
      " [   0    0    0 ...    0  397   65]\n",
      " [   0    0    0 ...    0    0   11]\n",
      " ...\n",
      " [   0    0    0 ...  372   10    3]\n",
      " [   0    0    0 ...   24  542    4]\n",
      " [   0    0    0 ... 2424  199  657]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07dbd60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9515129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20610 6871 20610 6871\n"
     ]
    }
   ],
   "source": [
    "#Splittting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)\n",
    "print (len(X_train),len(X_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372a949",
   "metadata": {},
   "source": [
    "# **SINGLE LSTM LAYER MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aedbdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.8192 - accuracy: 0.6428\n",
      "Epoch 1: val_accuracy improved from -inf to 0.70878, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 71s 107ms/step - loss: 0.8192 - accuracy: 0.6428 - val_loss: 0.6719 - val_accuracy: 0.7088\n",
      "Epoch 2/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.5850 - accuracy: 0.7658\n",
      "Epoch 2: val_accuracy improved from 0.70878 to 0.78940, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 56s 87ms/step - loss: 0.5850 - accuracy: 0.7658 - val_loss: 0.5337 - val_accuracy: 0.7894\n",
      "Epoch 3/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.5095 - accuracy: 0.8016\n",
      "Epoch 3: val_accuracy improved from 0.78940 to 0.80105, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 57s 89ms/step - loss: 0.5095 - accuracy: 0.8016 - val_loss: 0.5208 - val_accuracy: 0.8010\n",
      "Epoch 4/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4756 - accuracy: 0.8187\n",
      "Epoch 4: val_accuracy improved from 0.80105 to 0.80702, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 57s 88ms/step - loss: 0.4756 - accuracy: 0.8187 - val_loss: 0.4859 - val_accuracy: 0.8070\n",
      "Epoch 5/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4543 - accuracy: 0.8276\n",
      "Epoch 5: val_accuracy improved from 0.80702 to 0.82157, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 61s 94ms/step - loss: 0.4543 - accuracy: 0.8276 - val_loss: 0.4726 - val_accuracy: 0.8216\n",
      "Epoch 6/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.8323\n",
      "Epoch 6: val_accuracy improved from 0.82157 to 0.82317, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 59s 91ms/step - loss: 0.4425 - accuracy: 0.8323 - val_loss: 0.4650 - val_accuracy: 0.8232\n",
      "Epoch 7/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.8395\n",
      "Epoch 7: val_accuracy did not improve from 0.82317\n",
      "645/645 [==============================] - 54s 83ms/step - loss: 0.4277 - accuracy: 0.8395 - val_loss: 0.4597 - val_accuracy: 0.8227\n",
      "Epoch 8/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.8416\n",
      "Epoch 8: val_accuracy improved from 0.82317 to 0.82463, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 59s 92ms/step - loss: 0.4186 - accuracy: 0.8416 - val_loss: 0.4524 - val_accuracy: 0.8246\n",
      "Epoch 9/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4075 - accuracy: 0.8475\n",
      "Epoch 9: val_accuracy did not improve from 0.82463\n",
      "645/645 [==============================] - 60s 92ms/step - loss: 0.4075 - accuracy: 0.8475 - val_loss: 0.4594 - val_accuracy: 0.8179\n",
      "Epoch 10/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8489\n",
      "Epoch 10: val_accuracy did not improve from 0.82463\n",
      "645/645 [==============================] - 62s 96ms/step - loss: 0.3998 - accuracy: 0.8489 - val_loss: 0.4509 - val_accuracy: 0.8229\n",
      "Epoch 11/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3912 - accuracy: 0.8567\n",
      "Epoch 11: val_accuracy improved from 0.82463 to 0.82797, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 54s 84ms/step - loss: 0.3912 - accuracy: 0.8567 - val_loss: 0.4421 - val_accuracy: 0.8280\n",
      "Epoch 12/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.8574\n",
      "Epoch 12: val_accuracy improved from 0.82797 to 0.82855, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 59s 92ms/step - loss: 0.3875 - accuracy: 0.8574 - val_loss: 0.4459 - val_accuracy: 0.8286\n",
      "Epoch 13/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.8628\n",
      "Epoch 13: val_accuracy improved from 0.82855 to 0.83001, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 57s 88ms/step - loss: 0.3792 - accuracy: 0.8628 - val_loss: 0.4434 - val_accuracy: 0.8300\n",
      "Epoch 14/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3755 - accuracy: 0.8604\n",
      "Epoch 14: val_accuracy did not improve from 0.83001\n",
      "645/645 [==============================] - 54s 83ms/step - loss: 0.3755 - accuracy: 0.8604 - val_loss: 0.4421 - val_accuracy: 0.8290\n",
      "Epoch 15/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.8620\n",
      "Epoch 15: val_accuracy did not improve from 0.83001\n",
      "645/645 [==============================] - 57s 88ms/step - loss: 0.3715 - accuracy: 0.8620 - val_loss: 0.4461 - val_accuracy: 0.8291\n",
      "Epoch 16/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3685 - accuracy: 0.8640\n",
      "Epoch 16: val_accuracy improved from 0.83001 to 0.83088, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 60s 92ms/step - loss: 0.3685 - accuracy: 0.8640 - val_loss: 0.4413 - val_accuracy: 0.8309\n",
      "Epoch 17/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8673\n",
      "Epoch 17: val_accuracy did not improve from 0.83088\n",
      "645/645 [==============================] - 49s 75ms/step - loss: 0.3670 - accuracy: 0.8673 - val_loss: 0.4438 - val_accuracy: 0.8294\n",
      "Epoch 18/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.8655\n",
      "Epoch 18: val_accuracy improved from 0.83088 to 0.83481, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 57s 89ms/step - loss: 0.3612 - accuracy: 0.8655 - val_loss: 0.4340 - val_accuracy: 0.8348\n",
      "Epoch 19/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3608 - accuracy: 0.8675\n",
      "Epoch 19: val_accuracy improved from 0.83481 to 0.83554, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 54s 83ms/step - loss: 0.3608 - accuracy: 0.8675 - val_loss: 0.4266 - val_accuracy: 0.8355\n",
      "Epoch 20/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8693\n",
      "Epoch 20: val_accuracy did not improve from 0.83554\n",
      "645/645 [==============================] - 47s 73ms/step - loss: 0.3548 - accuracy: 0.8693 - val_loss: 0.4359 - val_accuracy: 0.8351\n",
      "Epoch 21/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.8716\n",
      "Epoch 21: val_accuracy did not improve from 0.83554\n",
      "645/645 [==============================] - 52s 81ms/step - loss: 0.3508 - accuracy: 0.8716 - val_loss: 0.4392 - val_accuracy: 0.8352\n",
      "Epoch 22/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.8715\n",
      "Epoch 22: val_accuracy did not improve from 0.83554\n",
      "645/645 [==============================] - 56s 87ms/step - loss: 0.3480 - accuracy: 0.8715 - val_loss: 0.4371 - val_accuracy: 0.8344\n",
      "Epoch 23/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8732\n",
      "Epoch 23: val_accuracy improved from 0.83554 to 0.83583, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 50s 77ms/step - loss: 0.3462 - accuracy: 0.8732 - val_loss: 0.4365 - val_accuracy: 0.8358\n",
      "Epoch 24/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8731\n",
      "Epoch 24: val_accuracy did not improve from 0.83583\n",
      "645/645 [==============================] - 59s 91ms/step - loss: 0.3464 - accuracy: 0.8731 - val_loss: 0.4403 - val_accuracy: 0.8341\n",
      "Epoch 25/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3447 - accuracy: 0.8733\n",
      "Epoch 25: val_accuracy did not improve from 0.83583\n",
      "645/645 [==============================] - 55s 85ms/step - loss: 0.3450 - accuracy: 0.8732 - val_loss: 0.4298 - val_accuracy: 0.8256\n",
      "Epoch 26/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.8730\n",
      "Epoch 26: val_accuracy improved from 0.83583 to 0.83729, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 56s 87ms/step - loss: 0.3432 - accuracy: 0.8730 - val_loss: 0.4325 - val_accuracy: 0.8373\n",
      "Epoch 27/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.8733\n",
      "Epoch 27: val_accuracy improved from 0.83729 to 0.83787, saving model to best_model1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645/645 [==============================] - 56s 87ms/step - loss: 0.3382 - accuracy: 0.8733 - val_loss: 0.4332 - val_accuracy: 0.8379\n",
      "Epoch 28/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3368 - accuracy: 0.8764\n",
      "Epoch 28: val_accuracy improved from 0.83787 to 0.83918, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 46s 71ms/step - loss: 0.3368 - accuracy: 0.8764 - val_loss: 0.4327 - val_accuracy: 0.8392\n",
      "Epoch 29/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3388 - accuracy: 0.8756\n",
      "Epoch 29: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 50s 77ms/step - loss: 0.3388 - accuracy: 0.8756 - val_loss: 0.4318 - val_accuracy: 0.8386\n",
      "Epoch 30/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.8777\n",
      "Epoch 30: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 62s 97ms/step - loss: 0.3348 - accuracy: 0.8777 - val_loss: 0.4364 - val_accuracy: 0.8360\n",
      "Epoch 31/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3350 - accuracy: 0.8763\n",
      "Epoch 31: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 55s 85ms/step - loss: 0.3350 - accuracy: 0.8763 - val_loss: 0.4380 - val_accuracy: 0.8385\n",
      "Epoch 32/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.8758\n",
      "Epoch 32: val_accuracy improved from 0.83918 to 0.84034, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 62s 96ms/step - loss: 0.3354 - accuracy: 0.8758 - val_loss: 0.4340 - val_accuracy: 0.8403\n",
      "Epoch 33/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.8761\n",
      "Epoch 33: val_accuracy did not improve from 0.84034\n",
      "645/645 [==============================] - 48s 75ms/step - loss: 0.3304 - accuracy: 0.8761 - val_loss: 0.4296 - val_accuracy: 0.8383\n",
      "Epoch 34/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8784\n",
      "Epoch 34: val_accuracy did not improve from 0.84034\n",
      "645/645 [==============================] - 57s 89ms/step - loss: 0.3301 - accuracy: 0.8785 - val_loss: 0.4304 - val_accuracy: 0.8383\n",
      "Epoch 35/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3292 - accuracy: 0.8786\n",
      "Epoch 35: val_accuracy did not improve from 0.84034\n",
      "645/645 [==============================] - 54s 83ms/step - loss: 0.3292 - accuracy: 0.8786 - val_loss: 0.4346 - val_accuracy: 0.8367\n",
      "Epoch 36/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3293 - accuracy: 0.8797\n",
      "Epoch 36: val_accuracy did not improve from 0.84034\n",
      "645/645 [==============================] - 56s 86ms/step - loss: 0.3293 - accuracy: 0.8797 - val_loss: 0.4296 - val_accuracy: 0.8387\n",
      "Epoch 37/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8808\n",
      "Epoch 37: val_accuracy did not improve from 0.84034\n",
      "645/645 [==============================] - 62s 96ms/step - loss: 0.3257 - accuracy: 0.8808 - val_loss: 0.4303 - val_accuracy: 0.8402\n",
      "Epoch 38/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3218 - accuracy: 0.8809\n",
      "Epoch 38: val_accuracy improved from 0.84034 to 0.84136, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 51s 79ms/step - loss: 0.3218 - accuracy: 0.8809 - val_loss: 0.4290 - val_accuracy: 0.8414\n",
      "Epoch 39/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.8807\n",
      "Epoch 39: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 261s 405ms/step - loss: 0.3229 - accuracy: 0.8807 - val_loss: 0.4422 - val_accuracy: 0.8390\n",
      "Epoch 40/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.8804\n",
      "Epoch 40: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 58s 90ms/step - loss: 0.3189 - accuracy: 0.8804 - val_loss: 0.4353 - val_accuracy: 0.8393\n",
      "Epoch 41/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.8825\n",
      "Epoch 41: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 52s 81ms/step - loss: 0.3199 - accuracy: 0.8825 - val_loss: 0.4333 - val_accuracy: 0.8352\n",
      "Epoch 42/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3170 - accuracy: 0.8842\n",
      "Epoch 42: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 47s 73ms/step - loss: 0.3170 - accuracy: 0.8842 - val_loss: 0.4427 - val_accuracy: 0.8387\n",
      "Epoch 43/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8816\n",
      "Epoch 43: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 51s 79ms/step - loss: 0.3202 - accuracy: 0.8816 - val_loss: 0.4339 - val_accuracy: 0.8409\n",
      "Epoch 44/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8847\n",
      "Epoch 44: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 47s 73ms/step - loss: 0.3139 - accuracy: 0.8847 - val_loss: 0.4414 - val_accuracy: 0.8405\n",
      "Epoch 45/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8845\n",
      "Epoch 45: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 50s 78ms/step - loss: 0.3132 - accuracy: 0.8845 - val_loss: 0.4413 - val_accuracy: 0.8377\n",
      "Epoch 46/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3137 - accuracy: 0.8854\n",
      "Epoch 46: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 52s 81ms/step - loss: 0.3137 - accuracy: 0.8854 - val_loss: 0.4393 - val_accuracy: 0.8411\n",
      "Epoch 47/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8874\n",
      "Epoch 47: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 45s 70ms/step - loss: 0.3089 - accuracy: 0.8874 - val_loss: 0.4337 - val_accuracy: 0.8409\n",
      "Epoch 48/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3092 - accuracy: 0.8864\n",
      "Epoch 48: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 49s 77ms/step - loss: 0.3092 - accuracy: 0.8864 - val_loss: 0.4382 - val_accuracy: 0.8403\n",
      "Epoch 49/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.8877\n",
      "Epoch 49: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 48s 75ms/step - loss: 0.3113 - accuracy: 0.8877 - val_loss: 0.4353 - val_accuracy: 0.8389\n",
      "Epoch 50/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3099 - accuracy: 0.8872\n",
      "Epoch 50: val_accuracy did not improve from 0.84136\n",
      "645/645 [==============================] - 52s 80ms/step - loss: 0.3099 - accuracy: 0.8872 - val_loss: 0.4342 - val_accuracy: 0.8385\n",
      "Epoch 51/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8880\n",
      "Epoch 51: val_accuracy improved from 0.84136 to 0.84296, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 53s 81ms/step - loss: 0.3067 - accuracy: 0.8880 - val_loss: 0.4380 - val_accuracy: 0.8430\n",
      "Epoch 52/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.8888\n",
      "Epoch 52: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 46s 71ms/step - loss: 0.3084 - accuracy: 0.8888 - val_loss: 0.4383 - val_accuracy: 0.8385\n",
      "Epoch 53/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.8879\n",
      "Epoch 53: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 51s 80ms/step - loss: 0.3077 - accuracy: 0.8879 - val_loss: 0.4364 - val_accuracy: 0.8401\n",
      "Epoch 54/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.8905\n",
      "Epoch 54: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 49s 76ms/step - loss: 0.3025 - accuracy: 0.8905 - val_loss: 0.4383 - val_accuracy: 0.8399\n",
      "Epoch 55/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.8898\n",
      "Epoch 55: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 52s 80ms/step - loss: 0.3036 - accuracy: 0.8898 - val_loss: 0.4379 - val_accuracy: 0.8408\n",
      "Epoch 56/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645/645 [==============================] - ETA: 0s - loss: 0.3016 - accuracy: 0.8892\n",
      "Epoch 56: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 52s 81ms/step - loss: 0.3016 - accuracy: 0.8892 - val_loss: 0.4403 - val_accuracy: 0.8373\n",
      "Epoch 57/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8905\n",
      "Epoch 57: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 50s 78ms/step - loss: 0.3006 - accuracy: 0.8905 - val_loss: 0.4378 - val_accuracy: 0.8395\n",
      "Epoch 58/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8895\n",
      "Epoch 58: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 51s 79ms/step - loss: 0.2996 - accuracy: 0.8894 - val_loss: 0.4437 - val_accuracy: 0.8377\n",
      "Epoch 59/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8931\n",
      "Epoch 59: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 63s 98ms/step - loss: 0.2975 - accuracy: 0.8931 - val_loss: 0.4372 - val_accuracy: 0.8382\n",
      "Epoch 60/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.8909\n",
      "Epoch 60: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 118s 183ms/step - loss: 0.3006 - accuracy: 0.8909 - val_loss: 0.4411 - val_accuracy: 0.8370\n",
      "Epoch 61/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2956 - accuracy: 0.8926\n",
      "Epoch 61: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 112s 174ms/step - loss: 0.2956 - accuracy: 0.8926 - val_loss: 0.4424 - val_accuracy: 0.8402\n",
      "Epoch 62/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8900\n",
      "Epoch 62: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 114s 176ms/step - loss: 0.3002 - accuracy: 0.8900 - val_loss: 0.4447 - val_accuracy: 0.8377\n",
      "Epoch 63/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.8925\n",
      "Epoch 63: val_accuracy did not improve from 0.84296\n",
      "645/645 [==============================] - 116s 180ms/step - loss: 0.2933 - accuracy: 0.8925 - val_loss: 0.4529 - val_accuracy: 0.8366\n",
      "Epoch 64/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2928 - accuracy: 0.8921\n",
      "Epoch 64: val_accuracy improved from 0.84296 to 0.84369, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 107s 166ms/step - loss: 0.2928 - accuracy: 0.8921 - val_loss: 0.4444 - val_accuracy: 0.8437\n",
      "Epoch 65/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2952 - accuracy: 0.8917\n",
      "Epoch 65: val_accuracy did not improve from 0.84369\n",
      "645/645 [==============================] - 104s 161ms/step - loss: 0.2952 - accuracy: 0.8917 - val_loss: 0.4446 - val_accuracy: 0.8431\n",
      "Epoch 66/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.8930\n",
      "Epoch 66: val_accuracy did not improve from 0.84369\n",
      "645/645 [==============================] - 96s 149ms/step - loss: 0.2904 - accuracy: 0.8930 - val_loss: 0.4430 - val_accuracy: 0.8411\n",
      "Epoch 67/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2920 - accuracy: 0.8932\n",
      "Epoch 67: val_accuracy did not improve from 0.84369\n",
      "645/645 [==============================] - 90s 140ms/step - loss: 0.2920 - accuracy: 0.8932 - val_loss: 0.4467 - val_accuracy: 0.8392\n",
      "Epoch 68/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8957\n",
      "Epoch 68: val_accuracy did not improve from 0.84369\n",
      "645/645 [==============================] - 90s 140ms/step - loss: 0.2883 - accuracy: 0.8957 - val_loss: 0.4395 - val_accuracy: 0.8386\n",
      "Epoch 69/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.8965\n",
      "Epoch 69: val_accuracy did not improve from 0.84369\n",
      "645/645 [==============================] - 74s 115ms/step - loss: 0.2895 - accuracy: 0.8965 - val_loss: 0.4483 - val_accuracy: 0.8364\n",
      "Epoch 70/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.8950\n",
      "Epoch 70: val_accuracy did not improve from 0.84369\n",
      "645/645 [==============================] - 76s 118ms/step - loss: 0.2885 - accuracy: 0.8950 - val_loss: 0.4437 - val_accuracy: 0.8390\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(layers.Embedding(max_words, 20))\n",
    "model1.add(layers.LSTM(15,dropout=0.5))\n",
    "model1.add(layers.Dense(3,activation='softmax'))\n",
    "\n",
    "\n",
    "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model1.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50fb264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.7923 - accuracy: 0.6524\n",
      "Epoch 1: val_accuracy improved from -inf to 0.76568, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 173s 254ms/step - loss: 0.7923 - accuracy: 0.6524 - val_loss: 0.6140 - val_accuracy: 0.7657\n",
      "Epoch 2/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.5606 - accuracy: 0.7754\n",
      "Epoch 2: val_accuracy did not improve from 0.76568\n",
      "645/645 [==============================] - 165s 255ms/step - loss: 0.5606 - accuracy: 0.7754 - val_loss: 0.5583 - val_accuracy: 0.7635\n",
      "Epoch 3/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4956 - accuracy: 0.8081\n",
      "Epoch 3: val_accuracy improved from 0.76568 to 0.80629, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 220s 341ms/step - loss: 0.4956 - accuracy: 0.8081 - val_loss: 0.4891 - val_accuracy: 0.8063\n",
      "Epoch 4/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4657 - accuracy: 0.8203\n",
      "Epoch 4: val_accuracy improved from 0.80629 to 0.81240, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 226s 350ms/step - loss: 0.4657 - accuracy: 0.8203 - val_loss: 0.4803 - val_accuracy: 0.8124\n",
      "Epoch 5/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4443 - accuracy: 0.8299\n",
      "Epoch 5: val_accuracy improved from 0.81240 to 0.82346, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 213s 330ms/step - loss: 0.4443 - accuracy: 0.8299 - val_loss: 0.4644 - val_accuracy: 0.8235\n",
      "Epoch 6/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4295 - accuracy: 0.8377\n",
      "Epoch 6: val_accuracy did not improve from 0.82346\n",
      "645/645 [==============================] - 229s 354ms/step - loss: 0.4295 - accuracy: 0.8377 - val_loss: 0.4635 - val_accuracy: 0.8229\n",
      "Epoch 7/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4181 - accuracy: 0.8418\n",
      "Epoch 7: val_accuracy did not improve from 0.82346\n",
      "645/645 [==============================] - 228s 354ms/step - loss: 0.4181 - accuracy: 0.8418 - val_loss: 0.4520 - val_accuracy: 0.8233\n",
      "Epoch 8/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4097 - accuracy: 0.8461\n",
      "Epoch 8: val_accuracy improved from 0.82346 to 0.82768, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 187s 291ms/step - loss: 0.4097 - accuracy: 0.8461 - val_loss: 0.4445 - val_accuracy: 0.8277\n",
      "Epoch 9/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3963 - accuracy: 0.8508\n",
      "Epoch 9: val_accuracy improved from 0.82768 to 0.82986, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 186s 289ms/step - loss: 0.3963 - accuracy: 0.8508 - val_loss: 0.4403 - val_accuracy: 0.8299\n",
      "Epoch 10/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3907 - accuracy: 0.8528\n",
      "Epoch 10: val_accuracy improved from 0.82986 to 0.83045, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 226s 351ms/step - loss: 0.3907 - accuracy: 0.8528 - val_loss: 0.4406 - val_accuracy: 0.8304\n",
      "Epoch 11/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3813 - accuracy: 0.8559\n",
      "Epoch 11: val_accuracy improved from 0.83045 to 0.83190, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 218s 338ms/step - loss: 0.3813 - accuracy: 0.8559 - val_loss: 0.4341 - val_accuracy: 0.8319\n",
      "Epoch 12/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.8584\n",
      "Epoch 12: val_accuracy improved from 0.83190 to 0.83336, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 229s 355ms/step - loss: 0.3791 - accuracy: 0.8584 - val_loss: 0.4307 - val_accuracy: 0.8334\n",
      "Epoch 13/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8618\n",
      "Epoch 13: val_accuracy did not improve from 0.83336\n",
      "645/645 [==============================] - 215s 333ms/step - loss: 0.3696 - accuracy: 0.8618 - val_loss: 0.4357 - val_accuracy: 0.8325\n",
      "Epoch 14/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3664 - accuracy: 0.8656\n",
      "Epoch 14: val_accuracy improved from 0.83336 to 0.83452, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 197s 305ms/step - loss: 0.3664 - accuracy: 0.8656 - val_loss: 0.4281 - val_accuracy: 0.8345\n",
      "Epoch 15/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3615 - accuracy: 0.8653\n",
      "Epoch 15: val_accuracy improved from 0.83452 to 0.83496, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 227s 352ms/step - loss: 0.3615 - accuracy: 0.8653 - val_loss: 0.4294 - val_accuracy: 0.8350\n",
      "Epoch 16/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3558 - accuracy: 0.8689\n",
      "Epoch 16: val_accuracy did not improve from 0.83496\n",
      "645/645 [==============================] - 197s 306ms/step - loss: 0.3558 - accuracy: 0.8689 - val_loss: 0.4338 - val_accuracy: 0.8347\n",
      "Epoch 17/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3534 - accuracy: 0.8681\n",
      "Epoch 17: val_accuracy improved from 0.83496 to 0.83627, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 222s 344ms/step - loss: 0.3534 - accuracy: 0.8681 - val_loss: 0.4280 - val_accuracy: 0.8363\n",
      "Epoch 18/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3489 - accuracy: 0.8689\n",
      "Epoch 18: val_accuracy improved from 0.83627 to 0.83816, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 173s 268ms/step - loss: 0.3489 - accuracy: 0.8689 - val_loss: 0.4274 - val_accuracy: 0.8382\n",
      "Epoch 19/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.8744\n",
      "Epoch 19: val_accuracy did not improve from 0.83816\n",
      "645/645 [==============================] - 205s 318ms/step - loss: 0.3437 - accuracy: 0.8744 - val_loss: 0.4236 - val_accuracy: 0.8374\n",
      "Epoch 20/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8742\n",
      "Epoch 20: val_accuracy improved from 0.83816 to 0.83947, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 201s 312ms/step - loss: 0.3444 - accuracy: 0.8742 - val_loss: 0.4262 - val_accuracy: 0.8395\n",
      "Epoch 21/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.8749\n",
      "Epoch 21: val_accuracy did not improve from 0.83947\n",
      "645/645 [==============================] - 244s 378ms/step - loss: 0.3405 - accuracy: 0.8749 - val_loss: 0.4232 - val_accuracy: 0.8389\n",
      "Epoch 22/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3352 - accuracy: 0.8762\n",
      "Epoch 22: val_accuracy improved from 0.83947 to 0.84078, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 206s 319ms/step - loss: 0.3352 - accuracy: 0.8762 - val_loss: 0.4329 - val_accuracy: 0.8408\n",
      "Epoch 23/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.8764\n",
      "Epoch 23: val_accuracy did not improve from 0.84078\n",
      "645/645 [==============================] - 196s 303ms/step - loss: 0.3371 - accuracy: 0.8764 - val_loss: 0.4235 - val_accuracy: 0.8398\n",
      "Epoch 24/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3324 - accuracy: 0.8770\n",
      "Epoch 24: val_accuracy did not improve from 0.84078\n",
      "645/645 [==============================] - 224s 348ms/step - loss: 0.3324 - accuracy: 0.8770 - val_loss: 0.4236 - val_accuracy: 0.8401\n",
      "Epoch 25/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.8787\n",
      "Epoch 25: val_accuracy did not improve from 0.84078\n",
      "645/645 [==============================] - 234s 363ms/step - loss: 0.3297 - accuracy: 0.8787 - val_loss: 0.4282 - val_accuracy: 0.8371\n",
      "Epoch 26/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.8789\n",
      "Epoch 26: val_accuracy did not improve from 0.84078\n",
      "645/645 [==============================] - 193s 299ms/step - loss: 0.3308 - accuracy: 0.8789 - val_loss: 0.4223 - val_accuracy: 0.8382\n",
      "Epoch 27/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8787\n",
      "Epoch 27: val_accuracy did not improve from 0.84078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645/645 [==============================] - 180s 280ms/step - loss: 0.3270 - accuracy: 0.8787 - val_loss: 0.4237 - val_accuracy: 0.8396\n",
      "Epoch 28/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3264 - accuracy: 0.8787\n",
      "Epoch 28: val_accuracy did not improve from 0.84078\n",
      "645/645 [==============================] - 220s 341ms/step - loss: 0.3264 - accuracy: 0.8787 - val_loss: 0.4225 - val_accuracy: 0.8398\n",
      "Epoch 29/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.8826\n",
      "Epoch 29: val_accuracy did not improve from 0.84078\n",
      "645/645 [==============================] - 155s 240ms/step - loss: 0.3239 - accuracy: 0.8826 - val_loss: 0.4245 - val_accuracy: 0.8393\n",
      "Epoch 30/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.8819\n",
      "Epoch 30: val_accuracy did not improve from 0.84078\n",
      "645/645 [==============================] - 149s 230ms/step - loss: 0.3209 - accuracy: 0.8819 - val_loss: 0.4251 - val_accuracy: 0.8398\n",
      "Epoch 31/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3189 - accuracy: 0.8836\n",
      "Epoch 31: val_accuracy did not improve from 0.84078\n",
      "645/645 [==============================] - 145s 225ms/step - loss: 0.3189 - accuracy: 0.8836 - val_loss: 0.4301 - val_accuracy: 0.8363\n",
      "Epoch 32/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.8850\n",
      "Epoch 32: val_accuracy improved from 0.84078 to 0.84122, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 143s 221ms/step - loss: 0.3176 - accuracy: 0.8850 - val_loss: 0.4238 - val_accuracy: 0.8412\n",
      "Epoch 33/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.8868\n",
      "Epoch 33: val_accuracy did not improve from 0.84122\n",
      "645/645 [==============================] - 106s 163ms/step - loss: 0.3138 - accuracy: 0.8868 - val_loss: 0.4216 - val_accuracy: 0.8399\n",
      "Epoch 34/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.8890\n",
      "Epoch 34: val_accuracy did not improve from 0.84122\n",
      "645/645 [==============================] - 132s 204ms/step - loss: 0.3104 - accuracy: 0.8890 - val_loss: 0.4264 - val_accuracy: 0.8390\n",
      "Epoch 35/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3134 - accuracy: 0.8851\n",
      "Epoch 35: val_accuracy did not improve from 0.84122\n",
      "645/645 [==============================] - 66s 103ms/step - loss: 0.3134 - accuracy: 0.8851 - val_loss: 0.4251 - val_accuracy: 0.8409\n",
      "Epoch 36/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.8890\n",
      "Epoch 36: val_accuracy improved from 0.84122 to 0.84209, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 60s 93ms/step - loss: 0.3118 - accuracy: 0.8890 - val_loss: 0.4257 - val_accuracy: 0.8421\n",
      "Epoch 37/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3107 - accuracy: 0.8876\n",
      "Epoch 37: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 79s 122ms/step - loss: 0.3107 - accuracy: 0.8876 - val_loss: 0.4293 - val_accuracy: 0.8399\n",
      "Epoch 38/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.8875\n",
      "Epoch 38: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 80s 124ms/step - loss: 0.3078 - accuracy: 0.8875 - val_loss: 0.4299 - val_accuracy: 0.8415\n",
      "Epoch 39/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3058 - accuracy: 0.8889\n",
      "Epoch 39: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 80s 123ms/step - loss: 0.3058 - accuracy: 0.8889 - val_loss: 0.4353 - val_accuracy: 0.8369\n",
      "Epoch 40/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.8909\n",
      "Epoch 40: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 88s 137ms/step - loss: 0.2980 - accuracy: 0.8909 - val_loss: 0.4261 - val_accuracy: 0.8398\n",
      "Epoch 41/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.8911\n",
      "Epoch 41: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 82s 127ms/step - loss: 0.3031 - accuracy: 0.8911 - val_loss: 0.4318 - val_accuracy: 0.8396\n",
      "Epoch 42/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2991 - accuracy: 0.8920\n",
      "Epoch 42: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.2991 - accuracy: 0.8920 - val_loss: 0.4367 - val_accuracy: 0.8386\n",
      "Epoch 43/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.8914\n",
      "Epoch 43: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 89s 137ms/step - loss: 0.2974 - accuracy: 0.8914 - val_loss: 0.4338 - val_accuracy: 0.8408\n",
      "Epoch 44/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.8928\n",
      "Epoch 44: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 90s 139ms/step - loss: 0.2973 - accuracy: 0.8928 - val_loss: 0.4503 - val_accuracy: 0.8367\n",
      "Epoch 45/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8939\n",
      "Epoch 45: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 106s 164ms/step - loss: 0.2938 - accuracy: 0.8939 - val_loss: 0.4343 - val_accuracy: 0.8376\n",
      "Epoch 46/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.8948\n",
      "Epoch 46: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.2933 - accuracy: 0.8948 - val_loss: 0.4366 - val_accuracy: 0.8411\n",
      "Epoch 47/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.8937\n",
      "Epoch 47: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 85s 132ms/step - loss: 0.2933 - accuracy: 0.8937 - val_loss: 0.4287 - val_accuracy: 0.8398\n",
      "Epoch 48/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.8947\n",
      "Epoch 48: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 86s 134ms/step - loss: 0.2912 - accuracy: 0.8947 - val_loss: 0.4318 - val_accuracy: 0.8387\n",
      "Epoch 49/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.8975\n",
      "Epoch 49: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 89s 137ms/step - loss: 0.2914 - accuracy: 0.8975 - val_loss: 0.4418 - val_accuracy: 0.8411\n",
      "Epoch 50/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.8952\n",
      "Epoch 50: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 79s 123ms/step - loss: 0.2901 - accuracy: 0.8952 - val_loss: 0.4344 - val_accuracy: 0.8398\n",
      "Epoch 51/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.8977\n",
      "Epoch 51: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 92s 143ms/step - loss: 0.2862 - accuracy: 0.8977 - val_loss: 0.4382 - val_accuracy: 0.8403\n",
      "Epoch 52/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.8973\n",
      "Epoch 52: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 96s 149ms/step - loss: 0.2827 - accuracy: 0.8973 - val_loss: 0.4476 - val_accuracy: 0.8402\n",
      "Epoch 53/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.8982\n",
      "Epoch 53: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 83s 129ms/step - loss: 0.2835 - accuracy: 0.8982 - val_loss: 0.4493 - val_accuracy: 0.8411\n",
      "Epoch 54/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.8987\n",
      "Epoch 54: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 89s 138ms/step - loss: 0.2813 - accuracy: 0.8987 - val_loss: 0.4444 - val_accuracy: 0.8412\n",
      "Epoch 55/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.8984\n",
      "Epoch 55: val_accuracy did not improve from 0.84209\n",
      "645/645 [==============================] - 79s 122ms/step - loss: 0.2843 - accuracy: 0.8984 - val_loss: 0.4432 - val_accuracy: 0.8376\n",
      "Epoch 56/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645/645 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.8969\n",
      "Epoch 56: val_accuracy improved from 0.84209 to 0.84398, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 83s 129ms/step - loss: 0.2836 - accuracy: 0.8969 - val_loss: 0.4392 - val_accuracy: 0.8440\n",
      "Epoch 57/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.8983\n",
      "Epoch 57: val_accuracy did not improve from 0.84398\n",
      "645/645 [==============================] - 84s 130ms/step - loss: 0.2815 - accuracy: 0.8983 - val_loss: 0.4456 - val_accuracy: 0.8415\n",
      "Epoch 58/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.9014\n",
      "Epoch 58: val_accuracy did not improve from 0.84398\n",
      "645/645 [==============================] - 83s 129ms/step - loss: 0.2756 - accuracy: 0.9014 - val_loss: 0.4561 - val_accuracy: 0.8430\n",
      "Epoch 59/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 0.9003\n",
      "Epoch 59: val_accuracy did not improve from 0.84398\n",
      "645/645 [==============================] - 85s 132ms/step - loss: 0.2805 - accuracy: 0.9003 - val_loss: 0.4434 - val_accuracy: 0.8408\n",
      "Epoch 60/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2778 - accuracy: 0.9002\n",
      "Epoch 60: val_accuracy did not improve from 0.84398\n",
      "645/645 [==============================] - 96s 149ms/step - loss: 0.2778 - accuracy: 0.9002 - val_loss: 0.4511 - val_accuracy: 0.8427\n",
      "Epoch 61/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 0.9010\n",
      "Epoch 61: val_accuracy did not improve from 0.84398\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.2781 - accuracy: 0.9010 - val_loss: 0.4499 - val_accuracy: 0.8403\n",
      "Epoch 62/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9003\n",
      "Epoch 62: val_accuracy improved from 0.84398 to 0.84456, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 94s 146ms/step - loss: 0.2720 - accuracy: 0.9003 - val_loss: 0.4567 - val_accuracy: 0.8446\n",
      "Epoch 63/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9019\n",
      "Epoch 63: val_accuracy did not improve from 0.84456\n",
      "645/645 [==============================] - 84s 130ms/step - loss: 0.2733 - accuracy: 0.9019 - val_loss: 0.4495 - val_accuracy: 0.8389\n",
      "Epoch 64/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9035\n",
      "Epoch 64: val_accuracy did not improve from 0.84456\n",
      "645/645 [==============================] - 95s 148ms/step - loss: 0.2703 - accuracy: 0.9035 - val_loss: 0.4505 - val_accuracy: 0.8408\n",
      "Epoch 65/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9052\n",
      "Epoch 65: val_accuracy did not improve from 0.84456\n",
      "645/645 [==============================] - 78s 121ms/step - loss: 0.2684 - accuracy: 0.9052 - val_loss: 0.4497 - val_accuracy: 0.8405\n",
      "Epoch 66/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.9033\n",
      "Epoch 66: val_accuracy did not improve from 0.84456\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.2687 - accuracy: 0.9033 - val_loss: 0.4464 - val_accuracy: 0.8415\n",
      "Epoch 67/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2688 - accuracy: 0.9028\n",
      "Epoch 67: val_accuracy did not improve from 0.84456\n",
      "645/645 [==============================] - 79s 123ms/step - loss: 0.2688 - accuracy: 0.9028 - val_loss: 0.4529 - val_accuracy: 0.8409\n",
      "Epoch 68/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9040\n",
      "Epoch 68: val_accuracy did not improve from 0.84456\n",
      "645/645 [==============================] - 79s 122ms/step - loss: 0.2703 - accuracy: 0.9040 - val_loss: 0.4541 - val_accuracy: 0.8399\n",
      "Epoch 69/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.9048\n",
      "Epoch 69: val_accuracy did not improve from 0.84456\n",
      "645/645 [==============================] - 89s 138ms/step - loss: 0.2695 - accuracy: 0.9048 - val_loss: 0.4540 - val_accuracy: 0.8415\n",
      "Epoch 70/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9045\n",
      "Epoch 70: val_accuracy did not improve from 0.84456\n",
      "645/645 [==============================] - 85s 132ms/step - loss: 0.2655 - accuracy: 0.9045 - val_loss: 0.4501 - val_accuracy: 0.8430\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
    "model2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
    "model2.add(layers.Dense(3,activation='softmax'))\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model2.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "987e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load the best model obtained during training\n",
    "best_model = keras.models.load_model(\"best_model2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "423dacdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 - 7s - loss: 0.4567 - accuracy: 0.8446 - 7s/epoch - 31ms/step\n",
      "Model accuracy:  0.8445640802383423\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Model accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1a92ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 7s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04f27a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test.argmax(axis=1), np.around(predictions, decimals=0).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "747976b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "conf_matrix = pd.DataFrame(matrix, index = ['Neutral','Negative','Positive'],columns = ['Neutral','Negative','Positive'])\n",
    "#Normalizing\n",
    "conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (15,15))\n",
    "sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e917b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = ['Neutral','Negative','Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a801f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['The boy was slow'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96c0849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['she is excited'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72f72c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c84bc3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pyttsx3 engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set the voice to use\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id) # use a female voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f64bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_with_emotion(text):\n",
    "    lines = text.split(\".\")\n",
    "    for line in lines:\n",
    "    # Analyze the sentiment of the text\n",
    "        sequence = tokenizer.texts_to_sequences([line])\n",
    "        test = pad_sequences(sequence, maxlen=max_len)\n",
    "        s=sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]\n",
    "\n",
    "        # Modify the speech parameters based on the sentiment\n",
    "        if s=='Positive':\n",
    "            # Positive sentiment\n",
    "            engine.setProperty('rate', 175) # increase the speed\n",
    "            engine.setProperty('pitch', 215) # increase the pitch\n",
    "        elif s=='Negative':\n",
    "            # Negative sentiment\n",
    "            engine.setProperty('rate', 100) # decrease the speed\n",
    "            engine.setProperty('pitch', 20) # lower the pitch\n",
    "        else:\n",
    "            # Neutral sentiment\n",
    "            engine.setProperty('rate', 125) # default settings\n",
    "            engine.setProperty('pitch', 100)\n",
    "\n",
    "        # Speak the text\n",
    "        engine.say(line)\n",
    "        engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e63771cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "text=\"The boy is happy . She is crying . I am angry\"\n",
    "speak_with_emotion(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "153fe1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "Tokenizer saved\n"
     ]
    }
   ],
   "source": [
    "#Saving weights and tokenizer so we can reduce training time on SageMaker\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = best_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "best_model.save_weights(\"model-weights.h5\")\n",
    "print(\"Model saved\")\n",
    "\n",
    "# saving tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('Tokenizer saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcc1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
